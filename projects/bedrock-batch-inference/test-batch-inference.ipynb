{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to unit test batch inference with bedrock\n",
    "\n",
    "(following guide [here](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference.html))\n",
    "\n",
    "\n",
    "Note, minimum input jsonl file size is 1MB for batch inference to work, even though [this quotas page](https://docs.aws.amazon.com/bedrock/latest/userguide/quotas.html#quotas-batch) says 20MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install python bedrock SDK from reinvent (downloaded as a zip file) [[link](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference-example.html)][[link](https://docs.aws.amazon.com/bedrock/latest/userguide/api-setup.html#api-sdk)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p tmp-boto-install\n",
    "!unzip /home/ubuntu/bedrock-python-sdk-reinvent.zip -d tmp-boto-install\n",
    "!pip install tmp-boto-install/botocore-1.32.4-py3-none-any.whl\n",
    "!pip install tmp-boto-install/boto3-1.29.4-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import boto3\n",
    "\n",
    "BUCKET_NAME = \"test-tmp-data-upload-bucket\"\n",
    "INPUT_FILE_NAME = \"test-batch-inference-data.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single prompt inference, to make sure connections work\n",
    "def test_single_prompt():\n",
    "    brt = boto3.client(service_name=\"bedrock-runtime\")\n",
    "\n",
    "    body = json.dumps(\n",
    "        {\n",
    "            \"prompt\": \"\\n\\nHuman: explain black holes to 8th graders\\n\\nAssistant:\",\n",
    "            \"max_tokens_to_sample\": 300,\n",
    "            \"temperature\": 0.1,\n",
    "            \"top_p\": 0.9,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    modelId = \"anthropic.claude-v2\"\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "\n",
    "    response = brt.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "    # text\n",
    "    print(response_body.get(\"completion\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_single_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For batch inference, you need to provide your inputs in an s3 bucket as a jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bucket_and_upload_file(bucket_name, file_name, object_name=None):\n",
    "    # Create S3 client\n",
    "    s3 = boto3.client(\"s3\")\n",
    "\n",
    "    # Create the bucket\n",
    "    try:\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "    except s3.exceptions.BucketAlreadyOwnedByYou:\n",
    "        print(f\"Bucket {bucket_name} already exists.\")\n",
    "\n",
    "    # Upload the file (object_name is the name as appears in s3)\n",
    "    if not object_name:\n",
    "        object_name = file_name\n",
    "\n",
    "    s3.upload_file(file_name, bucket_name, object_name)\n",
    "    print(f\"File uploaded to {bucket_name}/{object_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum input file size for batch inference is 1MB, so let's make up 2k examples asking the model to translate english to spanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/sandbox/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# !pip install datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "squad_dataset = load_dataset(\"squad\")  # 87599 rows in train\n",
    "\n",
    "\n",
    "def create_squad_prompts(dataset, nprompts=5) -> list[str]:\n",
    "    prompts = []\n",
    "    for i in range(nprompts):\n",
    "        prompts.append(\n",
    "            f\"\\n\\nHuman: Convert the following text to spanish: <text> {dataset['train'][i]['context']} \"\n",
    "            \"</text>\\n\\nAssistant: The text converted to spanish is: \"\n",
    "        )\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = create_squad_prompts(squad_dataset, nprompts=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a large jsonl file (min 1MB) for batch inference\n",
    "def create_jsonl_file(prompts: list, out_file_name: str):\n",
    "    # recordId is optional (will be assigned if empty)\n",
    "    # modelInput is what was the \"body\" in single-inference\n",
    "\n",
    "    with open(out_file_name, \"w\") as f:\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            f.write(\n",
    "                json.dumps(\n",
    "                    {\n",
    "                        \"recordId\": str(i),\n",
    "                        \"modelInput\": {\n",
    "                            \"prompt\": prompt,\n",
    "                            \"max_tokens_to_sample\": 1000,\n",
    "                            \"temperature\": 0.1,\n",
    "                            \"top_p\": 0.9,\n",
    "                        },\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "            f.write(\"\\n\")\n",
    "    print(f\"Created {out_file_name} with {i+1} lines.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created test-batch-inference-data.jsonl with 1999 lines.\n"
     ]
    }
   ],
   "source": [
    "create_jsonl_file(prompts, INPUT_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1M\ttest-batch-inference-data.jsonl\n"
     ]
    }
   ],
   "source": [
    "!du -hs $INPUT_FILE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the jsonl file to an s3 bucket where the bedrock batch job can grab it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded to test-tmp-data-upload-bucket/test-batch-inference-data.jsonl\n"
     ]
    }
   ],
   "source": [
    "create_bucket_and_upload_file(\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    file_name=INPUT_FILE_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the batch job, grab the jobArn identifier (used to check status, find results, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit batch job\n",
    "\n",
    "bedrock = boto3.client(service_name=\"bedrock\")\n",
    "\n",
    "inputDataConfig = {\n",
    "    \"s3InputDataConfig\": {\"s3Uri\": f\"s3://{BUCKET_NAME}/{INPUT_FILE_NAME}\"}\n",
    "}\n",
    "\n",
    "outputDataConfig = {\"s3OutputDataConfig\": {\"s3Uri\": f\"s3://{BUCKET_NAME}/\"}}\n",
    "\n",
    "response = bedrock.create_model_invocation_job(\n",
    "    roleArn=\"arn:aws:iam::339712833620:role/kaleko-test-ec2-bedrock-role\",\n",
    "    modelId=\"anthropic.claude-v2\",\n",
    "    jobName=\"my-batch-job-try7\",\n",
    "    inputDataConfig=inputDataConfig,\n",
    "    outputDataConfig=outputDataConfig,\n",
    ")\n",
    "\n",
    "jobArn = response.get(\"jobArn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'eaeeba7d-bd2e-4098-b516-5ccb87c5e7af',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Wed, 06 Mar 2024 22:16:50 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '597',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'eaeeba7d-bd2e-4098-b516-5ccb87c5e7af'},\n",
       "  'RetryAttempts': 0},\n",
       " 'jobArn': 'arn:aws:bedrock:us-east-1:339712833620:model-invocation-job/z8sje076r0pa',\n",
       " 'jobName': 'my-batch-job-try7',\n",
       " 'modelId': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:0',\n",
       " 'status': 'Completed',\n",
       " 'submitTime': datetime.datetime(2024, 3, 6, 20, 53, 50, 31000, tzinfo=tzlocal()),\n",
       " 'lastModifiedTime': datetime.datetime(2024, 3, 6, 21, 23, 43, 933000, tzinfo=tzlocal()),\n",
       " 'inputDataConfig': {'s3InputDataConfig': {'s3Uri': 's3://test-tmp-data-upload-bucket/test-batch-inference-data.jsonl'}},\n",
       " 'outputDataConfig': {'s3OutputDataConfig': {'s3Uri': 's3://test-tmp-data-upload-bucket/'}}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get status of job (keep polling this until it's done. 2k lines took 30 mins)\n",
    "bedrock.get_model_invocation_job(jobIdentifier=jobArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the artifacts the job created from the output s3 bucket (path within bucket defaults to the jobArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the created jsonl file to have a look, and the manifest that gets created\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "OUT_s3_DIR = jobArn.split(\"/\")[-1]\n",
    "!mkdir -p $OUT_s3_DIR\n",
    "\n",
    "MANIFEST_FILE = f\"{OUT_s3_DIR}/manifest.json.out\"\n",
    "OUTPUT_FILE = f\"{OUT_s3_DIR}/{INPUT_FILE_NAME}.out\"\n",
    "\n",
    "s3.download_file(BUCKET_NAME, MANIFEST_FILE, MANIFEST_FILE)\n",
    "s3.download_file(\n",
    "    BUCKET_NAME,\n",
    "    OUTPUT_FILE,\n",
    "    OUTPUT_FILE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"processedRecordCount\":2000,\"successRecordCount\":2000,\"errorRecordCount\":0,\"inputTokenCount\":436198,\"outputTokenCount\":574206}\n"
     ]
    }
   ],
   "source": [
    "!head $MANIFEST_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"modelInput\":{\"prompt\":\"\\n\\nHuman: Convert the following text to spanish: <text> Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \\\"Venite Ad Me Omnes\\\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary. </text>\\n\\nAssistant: The text converted to spanish is: \",\"max_tokens_to_sample\":1000,\"temperature\":0.1,\"top_p\":0.9},\"modelOutput\":{\"completion\":\"\\n\\n<text> Arquitectónicamente, la escuela tiene un carácter católico. En la cima de la cúpula dorada del Edificio Principal hay una estatua dorada de la Virgen María. Inmediatamente frente al Edificio Principal y de cara a él, hay una estatua de cobre de Cristo con los brazos levantados con la leyenda \\\"Venite Ad Me Omnes\\\". Junto al Edificio Principal está la Basílica del Sagrado Corazón. Inmediatamente detrás de la basílica está la Gruta, un lugar mariano de oración y reflexión. Es una réplica de la gruta de Lourdes, Francia, donde la Virgen María supuestamente se le apareció a Santa Bernardita Soubirous en 1858. Al final de la avenida principal (y en línea recta que conecta a través de 3 estatuas y la Cúpula Dorada), hay una simple y moderna estatua de piedra de María. </text>\",\"stop_reason\":\"stop_sequence\",\"stop\":\"\\n\\nHuman:\"},\"recordId\":\"0\"}\n"
     ]
    }
   ],
   "source": [
    "!head $OUTPUT_FILE -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute total cost from the manifest file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost: $17.27\n"
     ]
    }
   ],
   "source": [
    "# Compute total cost for curiosity\n",
    "mani = json.load(open(MANIFEST_FILE))\n",
    "IN_TOKEN_COST = 0.00800 / 1000.0\n",
    "OUT_TOKEN_COST = 0.02400 / 1000.0\n",
    "\n",
    "TOTAL_COST = (\n",
    "    mani[\"inputTokenCount\"] * IN_TOKEN_COST + mani[\"outputTokenCount\"] * OUT_TOKEN_COST\n",
    ")\n",
    "\n",
    "print(f\"Total cost: ${TOTAL_COST:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
