{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single prompt inference, to make sure connections work\n",
    "def invoke_claude_3_with_text(\n",
    "    prompt: str, model_id: str = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Invokes Anthropic Claude 3 Sonnet to run an inference using the input\n",
    "    provided in the request body.\n",
    "\n",
    "    :param prompt: The prompt that you want Claude 3 to complete.\n",
    "    :param model_id: The bedrock model ID to use for summarization\n",
    "    :return: Inference response from the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the Amazon Bedrock runtime client\n",
    "    client = boto3.client(service_name=\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "    try:\n",
    "        response = client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(\n",
    "                {\n",
    "                    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                    \"max_tokens\": 5000,\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [{\"type\": \"text\", \"text\": prompt}],\n",
    "                        }\n",
    "                    ],\n",
    "                    \"temperature\": 1,\n",
    "                    \"top_p\": 0.999,\n",
    "                    \"top_k\": 250,\n",
    "                }\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Process and print the response\n",
    "        result = json.loads(response.get(\"body\").read())\n",
    "        input_tokens = result[\"usage\"][\"input_tokens\"]\n",
    "        output_tokens = result[\"usage\"][\"output_tokens\"]\n",
    "        output_list = result.get(\"content\", [])\n",
    "\n",
    "        logger.debug(\"Invocation details:\")\n",
    "        logger.debug(f\"- The input length is {input_tokens} tokens.\")\n",
    "        logger.debug(f\"- The output length is {output_tokens} tokens.\")\n",
    "\n",
    "        logger.debug(f\"- The model returned {len(output_list)} response(s):\")\n",
    "        for output in output_list:\n",
    "            logger.debug(output[\"text\"])\n",
    "\n",
    "        return result\n",
    "\n",
    "    except ClientError as err:\n",
    "        logger.error(\n",
    "            \"Couldn't invoke Claude 3 Sonnet. Here's why: %s: %s\",\n",
    "            err.response[\"Error\"][\"Code\"],\n",
    "            err.response[\"Error\"][\"Message\"],\n",
    "        )\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_summary_from_transcript(\n",
    "    transcript: str, model_id: str = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    ") -> str:\n",
    "    FULL_PROMPT = (\n",
    "        \"I am a consultant building a generative AI application for a customer. \"\n",
    "        \"We are in the early phases of the engagement, and have conducted a \"\n",
    "        \"discovery workshop. The purpose of the workshop was to concretely \"\n",
    "        \"determine the scope of the POC, responsibilities, success metrics, \"\n",
    "        \"and next steps. I transcribed a recording of the workshop and will \"\n",
    "        \"provide the transcript to you below. Please write me a one page summary \"\n",
    "        \"of the meeting, emphasizing things like use case description, next steps, \"\n",
    "        \"potential obstacles identified, success criteria, and timelines. Here is \"\n",
    "        \"the meeting transcript:\\n\"\n",
    "        \"<transcript>\\n\"\n",
    "        f\"{transcript}\\n\"\n",
    "        \"</transcript>\"\n",
    "    )\n",
    "\n",
    "    res = invoke_claude_3_with_text(FULL_PROMPT)\n",
    "    output_list = res.get(\"content\", [])\n",
    "    assert len(output_list) == 1, f\"Output list has len {len(output_list)}, expected 1.\"\n",
    "    return output_list[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_transcript = \"hello thanks for attending the meeting about AMERICAN FOOTBALL!\"\n",
    "kazu = gen_summary_from_transcript(fake_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Unfortunately, the provided transcript does not contain any relevant information about a generative AI application or a discovery workshop. The transcript only says \"hello thanks for attending the meeting about AMERICAN FOOTBALL!\".\\n\\nWithout any actual details from the discovery workshop discussion, I cannot provide a meaningful one-page summary covering aspects such as the use case description, next steps, potential obstacles, success criteria, and timelines for the generative AI application project.\\n\\nPlease provide the full and accurate transcript of the discovery workshop to allow me to summarize the key points and outcomes effectively.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kazu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
