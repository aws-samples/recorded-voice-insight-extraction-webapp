{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bedrock Knowledge Base Retrieval and Generation for ReVIEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import sys\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "sys.path.append(\"../frontend/\")\n",
    "from components.bedrock_utils import get_bedrock_client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOUNDATION_MODEL = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "REGION_NAME = \"us-east-1\"\n",
    "KNOWLEDGE_BASE_ID = \"VX6KXQMHZ2\"\n",
    "NUM_CHUNKS = 5\n",
    "USERNAME = \"demouser\"\n",
    "MEDIA_NAME = None\n",
    "QUERY = \"Did they mention Nvidia?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-east-1\n",
      "Found credentials in shared credentials file: ~/.aws/credentials\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock_client._endpoint=bedrock-agent-runtime(https://bedrock-agent-runtime.us-east-1.amazonaws.com)\n",
      "Create new client\n",
      "  Using region: us-east-1\n",
      "Found credentials in shared credentials file: ~/.aws/credentials\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock_client._endpoint=bedrock-runtime(https://bedrock-runtime.us-east-1.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "# Used for retrieval\n",
    "bedrock_agent_runtime_client = get_bedrock_client(region=REGION_NAME, agent=True)\n",
    "\n",
    "# Used for generation\n",
    "bedrock_client = get_bedrock_client(region=REGION_NAME, agent=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(agent_client, query, username, media_name, num_chunks):\n",
    "    # Always filter on username to prevent people from querying other users' data\n",
    "    # Optionally filter on media name if user wants to chat with just one media file\n",
    "    username_filter = {\"equals\": {\"key\": \"username\", \"value\": username}}\n",
    "    if not MEDIA_NAME:\n",
    "        retrieval_filter = username_filter\n",
    "    else:\n",
    "        retrieval_filter = {\n",
    "            \"andAll\": [\n",
    "                username_filter,\n",
    "                {\"equals\": {\"key\": \"media_name\", \"value\": media_name}},\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    retrieval_config = {\n",
    "        \"vectorSearchConfiguration\": {\n",
    "            \"numberOfResults\": num_chunks,\n",
    "            \"filter\": retrieval_filter,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    res = agent_client.retrieve(\n",
    "        knowledgeBaseId=KNOWLEDGE_BASE_ID,\n",
    "        retrievalConfiguration=retrieval_config,\n",
    "        retrievalQuery={\"text\": query},\n",
    "    )\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = retrieve(\n",
    "    agent_client=bedrock_agent_runtime_client,\n",
    "    query=QUERY,\n",
    "    username=USERNAME,\n",
    "    media_name=MEDIA_NAME,\n",
    "    num_chunks=NUM_CHUNKS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 chunks retrieved.\n",
      "First chunk:\n",
      "  Text: [0] from scoping requirements to defining evaluati ...\n",
      "  Score: 0.3786281\n",
      "  Location: s3://kazu-dev-339712833620-assets/transcripts-txt/demouser/a88fb9b4-5253-40db-8f0d-6a34e9f1f00b.txt\n",
      "  Custom Meta: test-5min-vid.mp4\n"
     ]
    }
   ],
   "source": [
    "kazu = res[\"retrievalResults\"]\n",
    "print(f\"{len(kazu)} chunks retrieved.\")\n",
    "c0 = kazu[0]\n",
    "print(\"First chunk:\")\n",
    "print(f\"  Text: {c0['content']['text'][:50]} ...\")\n",
    "print(f\"  Score: {c0['score']}\")\n",
    "print(f\"  Location: {c0['location']['s3Location']['uri']}\")\n",
    "print(f\"  Custom Meta: {c0['metadata']['media_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chunks_string(retrieve_response: dict) -> str:\n",
    "    \"\"\"Build a single string from retrieved chunks like:\n",
    "    <chunk_1>\n",
    "    <media_name>\n",
    "    foo-bar-vid.mp4\n",
    "    </media_name>\n",
    "    <transcript>\n",
    "    [0] blah blah [12] blah blah blah\n",
    "    </transcript>\n",
    "    </chunk_1>\n",
    "    <chunk_2>\n",
    "    ...\n",
    "    \"\"\"\n",
    "    chunks_string = \"\"\n",
    "    for i, chunk in enumerate(retrieve_response[\"retrievalResults\"]):\n",
    "        chunks_string += f\"<chunk_{i+1}>\\n<media_name>\\n{chunk['metadata']['media_name']}\\n</media_name>\\n<transcript>\\n{chunk['content']['text']}\\n</transcript>\\n</chunk_{i+1}>\\n\\n\"\n",
    "    return chunks_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(br_client, model_id, query, retrieval_response, **kwargs) -> str:\n",
    "    SYSTEM_PROMPT = \"\"\"You are an intelligent AI which attempts to answer questions based on retrieved chunks of automatically generated transcripts.\"\"\"\n",
    "\n",
    "    MESSAGE_TEMPLATE = \"\"\"\n",
    "I will provide you with retrieved chunks of transcripts. The user will provide you with a question. Using only information in the provided transcript chunks, you will attempt to answer the user's question.\n",
    "\n",
    "Each chunk may or may not be relevant to answering the question. Each chunk will include a <media_name> block which contains the parent file that the transcript came from. Each line in the transcript chunk begins with an integer timestamp (in seconds) within square brackets, followed by a transcribed sentence. When answering the question, you will need to provide the timestamp you got the answer from.\n",
    "\n",
    "Here are the retrieved chunks of transcripts in numbered order:\n",
    "\n",
    "<transcript_chunks>\n",
    "{chunks}\n",
    "</transcript_chunks>\n",
    "\n",
    "When you answer the question, your answer must be a parsable json string. The json should have two keys. One key, \"answer\", is your answer to the user's question. The second key, \"citations\" is a list of dicts which contain a \"media_name\" key and a \"timestamp\" key, which correspond to the resources used to answer the question. For example, if you got your answer from only one chunk, then the \"citations\" list will be only one element long, with the media_name of the chunk from which you got the answer, and the relevant timestamp within that chunk's transcript. If you used information from three chunks, the \"citations\" list will be three elements long.\n",
    "\n",
    "If you are unable to answer the question using information provided in any of the chunks, your response should include no citations like this:\n",
    "{{\"answer\": \"I am unable to answer the question based on the provided media file(s).\", \"citations\": []}}\n",
    "\n",
    "Here is the user's question:\n",
    "<question>\n",
    "{query}\n",
    "</question>\n",
    "    \"\"\"\n",
    "\n",
    "    chunks_str = build_chunks_string(retrieval_response)\n",
    "    message_content = MESSAGE_TEMPLATE.format(query=query, chunks=chunks_str)\n",
    "\n",
    "    body = {\n",
    "        \"system\": SYSTEM_PROMPT,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": message_content}],\n",
    "        \"anthropic_version\": \"\",\n",
    "        **kwargs,\n",
    "    }\n",
    "    response = br_client.invoke_model(modelId=model_id, body=json.dumps(body))\n",
    "    response = json.loads(response[\"body\"].read().decode(\"utf-8\"))\n",
    "\n",
    "    return response[\"content\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "class Citation(BaseModel):\n",
    "    media_name: str\n",
    "    timestamp: int\n",
    "\n",
    "class LLMAnswer(BaseModel):\n",
    "    answer: str\n",
    "    citations: List[Citation]\n",
    "\n",
    "    def pprint(self):\n",
    "        print(f\"LLMAnswer:\\n Answer={self.answer}\\n Citations={self.citations}\")\n",
    "\n",
    "def parse_generation(generation_response: str) -> LLMAnswer:\n",
    "    llm_answer = LLMAnswer(**json.loads(generation_response))\n",
    "    return llm_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-east-1\n",
      "Found credentials in shared credentials file: ~/.aws/credentials\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock_client._endpoint=bedrock-agent-runtime(https://bedrock-agent-runtime.us-east-1.amazonaws.com)\n",
      "Create new client\n",
      "  Using region: us-east-1\n",
      "Found credentials in shared credentials file: ~/.aws/credentials\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock_client._endpoint=bedrock-runtime(https://bedrock-runtime.us-east-1.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# Full workflow example #\n",
    "#########################\n",
    "\n",
    "FOUNDATION_MODEL = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "REGION_NAME = \"us-east-1\"\n",
    "KNOWLEDGE_BASE_ID = \"VX6KXQMHZ2\"\n",
    "NUM_CHUNKS = 5\n",
    "USERNAME = \"demouser\"\n",
    "MEDIA_NAME = None\n",
    "\n",
    "query = \"What AWS services are mentioned?\"\n",
    "\n",
    "# Used for retrieval\n",
    "bedrock_agent_runtime_client = get_bedrock_client(region=REGION_NAME, agent=True)\n",
    "# Used for generation\n",
    "bedrock_client = get_bedrock_client(region=REGION_NAME, agent=False)\n",
    "\n",
    "retrieval_result = retrieve(\n",
    "    agent_client=bedrock_agent_runtime_client,\n",
    "    query=query,\n",
    "    username=USERNAME,\n",
    "    media_name=MEDIA_NAME,\n",
    "    num_chunks=NUM_CHUNKS,\n",
    ")\n",
    "\n",
    "generate_result = generate(\n",
    "    br_client=bedrock_client,\n",
    "    model_id=FOUNDATION_MODEL,\n",
    "    query=query,\n",
    "    retrieval_response=retrieval_result,\n",
    "    temperature=0.1,\n",
    "    max_tokens=200,\n",
    ")\n",
    "\n",
    "answer: LLMAnswer = parse_generation(generate_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMAnswer:\n",
      " Answer=The AWS services mentioned in the provided transcript chunks are Amazon Transcribe, Amazon SageMaker, AWS Trainium, AWS Inferentia, Amazon Bedrock, and Amazon Cognito.\n",
      " Citations=[Citation(media_name='test-vid1.mp4', timestamp=31), Citation(media_name='test-5min-vid.mp4', timestamp=63), Citation(media_name='test-5min-vid.mp4', timestamp=79), Citation(media_name='test-vid1.mp4', timestamp=31)]\n"
     ]
    }
   ],
   "source": [
    "answer.pprint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
