{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bedrock Knowledge Base Retrieval and Generation for ReVIEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import sys\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "sys.path.append(\"../frontend/\")\n",
    "from components.bedrock_utils import get_bedrock_client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOUNDATION_MODEL = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "REGION_NAME = \"us-east-1\"\n",
    "KNOWLEDGE_BASE_ID = \"P1YXM9ZYUA\"\n",
    "NUM_CHUNKS = 5\n",
    "USERNAME = \"demouser\"\n",
    "MEDIA_NAME = None\n",
    "QUERY = \"Did they mention Nvidia?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-east-1\n",
      "Found credentials in shared credentials file: ~/.aws/credentials\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock_client._endpoint=bedrock-agent-runtime(https://bedrock-agent-runtime.us-east-1.amazonaws.com)\n",
      "Create new client\n",
      "  Using region: us-east-1\n",
      "Found credentials in shared credentials file: ~/.aws/credentials\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock_client._endpoint=bedrock-runtime(https://bedrock-runtime.us-east-1.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "# Used for retrieval\n",
    "bedrock_agent_runtime_client = get_bedrock_client(region=REGION_NAME, agent=True)\n",
    "\n",
    "# Used for generation\n",
    "bedrock_client = get_bedrock_client(region=REGION_NAME, agent=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(agent_client, query, username, media_name, num_chunks):\n",
    "    # Always filter on username to prevent people from querying other users' data\n",
    "    # Optionally filter on media name if user wants to chat with just one media file\n",
    "    username_filter = {\"equals\": {\"key\": \"username\", \"value\": username}}\n",
    "    if not media_name:\n",
    "        retrieval_filter = username_filter\n",
    "    else:\n",
    "        retrieval_filter = {\n",
    "            \"andAll\": [\n",
    "                username_filter,\n",
    "                {\"equals\": {\"key\": \"media_name\", \"value\": media_name}},\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    retrieval_config = {\n",
    "        \"vectorSearchConfiguration\": {\n",
    "            \"numberOfResults\": num_chunks,\n",
    "            \"filter\": retrieval_filter,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    res = agent_client.retrieve(\n",
    "        knowledgeBaseId=KNOWLEDGE_BASE_ID,\n",
    "        retrievalConfiguration=retrieval_config,\n",
    "        retrievalQuery={\"text\": query},\n",
    "    )\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = retrieve(\n",
    "    agent_client=bedrock_agent_runtime_client,\n",
    "    query=QUERY,\n",
    "    username=USERNAME,\n",
    "    media_name=MEDIA_NAME,\n",
    "    num_chunks=NUM_CHUNKS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 chunks retrieved.\n",
      "First chunk:\n",
      "  Text: of the meeting. [117] If this is a recording of a  ...\n",
      "  Score: 0.36510268\n",
      "  Location: s3://review-dev-339712833620-assets/transcripts-txt/demouser/379ebdf7-53ab-4f4a-9a19-34b55bc33d1b.txt\n",
      "  Custom Meta: test-vid1.mp4\n"
     ]
    }
   ],
   "source": [
    "kazu = res[\"retrievalResults\"]\n",
    "print(f\"{len(kazu)} chunks retrieved.\")\n",
    "c0 = kazu[0]\n",
    "print(\"First chunk:\")\n",
    "print(f\"  Text: {c0['content']['text'][:50]} ...\")\n",
    "print(f\"  Score: {c0['score']}\")\n",
    "print(f\"  Location: {c0['location']['s3Location']['uri']}\")\n",
    "print(f\"  Custom Meta: {c0['metadata']['media_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chunks_string(retrieve_response: dict) -> str:\n",
    "    \"\"\"Build a single string from retrieved chunks like:\n",
    "    <chunk_1>\n",
    "    <media_name>\n",
    "    foo-bar-vid.mp4\n",
    "    </media_name>\n",
    "    <transcript>\n",
    "    [0] blah blah [12] blah blah blah\n",
    "    </transcript>\n",
    "    </chunk_1>\n",
    "    <chunk_2>\n",
    "    ...\n",
    "    \"\"\"\n",
    "    chunks_string = \"\"\n",
    "    for i, chunk in enumerate(retrieve_response[\"retrievalResults\"]):\n",
    "        chunks_string += f\"<chunk_{i+1}>\\n<media_name>\\n{chunk['metadata']['media_name']}\\n</media_name>\\n<transcript>\\n{chunk['content']['text']}\\n</transcript>\\n</chunk_{i+1}>\\n\\n\"\n",
    "    return chunks_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(br_client, model_id, query, retrieval_response, **kwargs) -> str:\n",
    "    SYSTEM_PROMPT = \"\"\"You are an intelligent AI which attempts to answer questions based on retrieved chunks of automatically generated transcripts.\"\"\"\n",
    "\n",
    "    MESSAGE_TEMPLATE = \"\"\"\n",
    "I will provide you with retrieved chunks of transcripts. The user will provide you with a question. Using only information in the provided transcript chunks, you will attempt to answer the user's question.\n",
    "\n",
    "Each chunk may or may not be relevant to answering the question. Each chunk will include a <media_name> block which contains the parent file that the transcript came from. Each line in the transcript chunk begins with an integer timestamp (in seconds) within square brackets, followed by a transcribed sentence. When answering the question, you will need to provide the timestamp you got the answer from.\n",
    "\n",
    "Here are the retrieved chunks of transcripts in numbered order:\n",
    "\n",
    "<transcript_chunks>\n",
    "{chunks}\n",
    "</transcript_chunks>\n",
    "\n",
    "When you answer the question, your answer must include a parsable json string contained within <json></json> tags. The json should have one top level key, \"answer\", whose value is a list. Each element in the list represents a portion of the full answer, and should have two keys: \"partial_answer\", is a part of your answer to the user's question, and \"citations\" which is a list of dicts which contain a \"media_name\" key and a \"timestamp\" key, which correspond to the resources used to answer that part of the question. For example, if you got this partial_answer from only one chunk, then the \"citations\" list will be only one element long, with the media_name of the chunk from which you got the partial_answer, and the relevant timestamp within that chunk's transcript. If you used information from three chunks for this partial_answer, the \"citations\" list will be three elements long. For multi-part answers, the partial_answer list will be multiple elements long.\n",
    "\n",
    "The final answer displayed to the user will be all of the partial_answers concatenated. Make sure that you format your partial answers appropriately. For example, if your response has two partial answers which are meant to be displayed as a comma separated list, the first partial_answer should be formatted like \"partial_answer\": \"The two partial answers are this\" and the second partial_answer should be formatted like \"partial_answer\": \", and this.\". Similarly, if your partial answers are meant to be a bulleted list, the first partial answer may look like \"partial_answer\": \"The partial answers are:\\\\n- First partial answer\" and \"partial_answer\": \"\\\\n- Second partial answer\". Note the newline character at the beginning of the second partial_answer for final display purposes.\n",
    "\n",
    "For example, if your answer is in two parts, the first part coming from two chunks, the second part coming from one chunk, your answer will have this structure:\n",
    "<json>\n",
    "{{\"answer\": [ {{\"partial_answer\": \"This is the first part to the answer.\", \"citations\": [{{\"media_name\": \"media_file_foo.mp4\", \"timestamp\": 123}}, {{\"media_name\": \"media_file_bar.mp4\", \"timestamp\": 345}}]}}, {{\"partial_answer\": \" This is the second part to the answer.\", \"citations\": [{{\"media_name\": \"blahblah.wav\", \"timestamp\": 83}}]}} ] }}\n",
    "</json>\n",
    "\n",
    "Notice the space at the beginning of the second partial_answer string, \" This is...\". That space is important so when the partial_answers get concatenated they will be readable, like \"This is the first part to the answer. This is the second...\"\n",
    "\n",
    "If you are unable to answer the question using information provided in any of the chunks, your response should include no citations like this:\n",
    "<json>\n",
    "{{\"answer\": [ {{\"partial_answer\": \"I am unable to answer the question based on the provided media file(s).\", \"citations\": []}} ] }}\n",
    "</json>\n",
    "\n",
    "Here is the user's question:\n",
    "<question>\n",
    "{query}\n",
    "</question>\n",
    "\n",
    "Now write your json response in <json> </json> brackets like explained above. Make sure the content between the brackets is json parsable, e.g. escaping \" marks inside of strings and so on.\n",
    "    \"\"\"\n",
    "\n",
    "    chunks_str = build_chunks_string(retrieval_response)\n",
    "    message_content = MESSAGE_TEMPLATE.format(query=query, chunks=chunks_str)\n",
    "\n",
    "    body = {\n",
    "        \"system\": SYSTEM_PROMPT,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": message_content}],\n",
    "        \"anthropic_version\": \"\",\n",
    "        **kwargs,\n",
    "    }\n",
    "    response = br_client.invoke_model(modelId=model_id, body=json.dumps(body))\n",
    "    response = json.loads(response[\"body\"].read().decode(\"utf-8\"))\n",
    "\n",
    "    return response[\"content\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "class Citation(BaseModel):\n",
    "    media_name: str\n",
    "    timestamp: int\n",
    "\n",
    "\n",
    "class PartialQAnswer(BaseModel):\n",
    "    partial_answer: str\n",
    "    citations: List[Citation]\n",
    "\n",
    "    def pprint(self):\n",
    "        print(f\"LLMAnswer:\\n Answer={self.answer}\\n Citations={self.citations}\")\n",
    "\n",
    "\n",
    "class FullQAnswer(BaseModel):\n",
    "    answer: List[PartialQAnswer]\n",
    "\n",
    "    @classmethod\n",
    "    def from_LLM_response(cls, generation_response: str) -> \"FullQAnswer\":\n",
    "        \"\"\"\n",
    "        Create a FullQAnswer instance from an LLM response string containing JSON data.\n",
    "\n",
    "        The JSON data should be enclosed between <json> and </json> tags.\n",
    "        \"\"\"\n",
    "        pattern = r\"<json>\\s*(.*?)\\s*</json>\"\n",
    "        matches = re.findall(pattern, generation_response, re.DOTALL)\n",
    "        if not matches:\n",
    "            raise ValueError(\"No JSON data found between <json> and </json> tags\")\n",
    "\n",
    "        match = matches[0].strip(\"\\n\")\n",
    "        try:\n",
    "            data = json.loads(match)\n",
    "            return cls(**data)\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"Invalid JSON data: {e}\")\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Error creating FullQAnswer instance: {e}\")\n",
    "\n",
    "    def pprint(self):\n",
    "        result = \"\"\n",
    "        citation_counter = 1\n",
    "        citations = []\n",
    "\n",
    "        for partial in self.answer:\n",
    "            result += partial.partial_answer\n",
    "\n",
    "            if partial.citations:\n",
    "                citation_refs = []\n",
    "                for citation in partial.citations:\n",
    "                    citation_refs.append(f\"[{citation_counter}]\")\n",
    "                    citations.append(\n",
    "                        f\"[{citation_counter}] http://fake_url.com/{citation.media_name}?start_time={citation.timestamp}\"\n",
    "                    )\n",
    "                    citation_counter += 1\n",
    "                result += \"\".join(citation_refs)\n",
    "\n",
    "            result += \"\\n\"\n",
    "\n",
    "        result += \"\\nCitations:\\n\"\n",
    "        result += \"\\n\".join(citations)\n",
    "\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-east-1\n",
      "Found credentials in shared credentials file: ~/.aws/credentials\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock_client._endpoint=bedrock-agent-runtime(https://bedrock-agent-runtime.us-east-1.amazonaws.com)\n",
      "Create new client\n",
      "  Using region: us-east-1\n",
      "Found credentials in shared credentials file: ~/.aws/credentials\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock_client._endpoint=bedrock-runtime(https://bedrock-runtime.us-east-1.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# Full workflow example #\n",
    "#########################\n",
    "\n",
    "FOUNDATION_MODEL = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "REGION_NAME = \"us-east-1\"\n",
    "KNOWLEDGE_BASE_ID = \"P1YXM9ZYUA\"\n",
    "NUM_CHUNKS = 5\n",
    "USERNAME = \"demouser\"\n",
    "MEDIA_NAME = None\n",
    "\n",
    "query = \"What AWS services are mentioned?\"\n",
    "# query = \"Do they mention Nvidia? Please provide exact quotes.\"\n",
    "\n",
    "# Used for retrieval\n",
    "bedrock_agent_runtime_client = get_bedrock_client(region=REGION_NAME, agent=True)\n",
    "# Used for generation\n",
    "bedrock_client = get_bedrock_client(region=REGION_NAME, agent=False)\n",
    "\n",
    "retrieval_result = retrieve(\n",
    "    agent_client=bedrock_agent_runtime_client,\n",
    "    query=query,\n",
    "    username=USERNAME,\n",
    "    media_name=MEDIA_NAME,\n",
    "    num_chunks=NUM_CHUNKS,\n",
    ")\n",
    "\n",
    "generate_result = generate(\n",
    "    br_client=bedrock_client,\n",
    "    model_id=FOUNDATION_MODEL,\n",
    "    query=query,\n",
    "    retrieval_response=retrieval_result,\n",
    "    temperature=0.1,\n",
    "    max_tokens=5000,\n",
    ")\n",
    "\n",
    "answer: FullQAnswer = FullQAnswer.from_LLM_response(generate_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following AWS services are mentioned:\n",
      "\n",
      "- Amazon Transcribe[1]\n",
      "\n",
      "- DynamoDB for job tracking[2]\n",
      "\n",
      "- Amazon Bedrock for large language models[3]\n",
      "\n",
      "- Amazon Cognito for user authentication[4]\n",
      "\n",
      "- Amazon SageMaker (mentioned as a feature announcement)[5]\n",
      "\n",
      "Citations:\n",
      "[1] http://fake_url.com/test-vid1.mp4?start_time=29\n",
      "[2] http://fake_url.com/test-vid1.mp4?start_time=31\n",
      "[3] http://fake_url.com/test-vid1.mp4?start_time=31\n",
      "[4] http://fake_url.com/test-vid1.mp4?start_time=31\n",
      "[5] http://fake_url.com/test-vid1.mp4?start_time=208\n"
     ]
    }
   ],
   "source": [
    "answer.pprint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
